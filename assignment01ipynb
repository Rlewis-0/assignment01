"""
NOTE: This is example code to show you how to organize the project. This code does not contain feature preprocessing.
"""

import sys
import pandas as pd
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix # TODO: learn how to use these two functions
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt




# 1. Load data
def load_data(path):
   df = pd.read_csv(path)
   # print(df)
   return df


def feature_processing(df):
    
    # print(df.info())
    # sys.exit()
   
    df = df.drop(columns=(['three_g']))
   
    # print(df.info())
    # sys.exit()
    
   #TODO: Feature engineering.
   # print("Number of features:", df.shape[1])
   # # Number of NaNs per column
   # print("\nNaNs per feature:\n", df.isna().sum())
   # # Total NaNs in the whole dataset
   # print("\nTotal NaNs in dataset:", df.isna().sum().sum())
    
   # data set has 20 features and there are no missing values
  
    return df


# 2. Dataset class
class MobilePriceDataset(Dataset):
   """
   You can directly use this function, we will discuss more about this function in Assignment 2.
   """
   def __init__(self, X, y):
       self.X = torch.tensor(X.values, dtype=torch.float32)
       self.y = torch.tensor(y.values, dtype=torch.long)


   def __len__(self):
       return len(self.X)


   def __getitem__(self, idx):
       return self.X[idx], self.y[idx]


# 3. MLP model
class MLPClassifier(nn.Module):
  # TODO: Here I built a very simple MLP, you need to build your own model.
  # i can add drop out
#    def __init__(self, input_dim, hidden_dims=[16, 2], output_dim=4):
    def __init__(self, input_dim, hidden_dims=[742, 128, 128, 32, 16], output_dim=4):
        super(MLPClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dims[0]),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dims[1], hidden_dims[2]),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dims[2], hidden_dims[3]),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dims[3], hidden_dims[4]),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dims[4], output_dim) # Do not apply softmax here!!!
        )


    def forward(self, x):
       return self.model(x)

def predict(model, X_test_loader, device):
       """
       Evaluate model on test data and return comprehensive metrics
       """
       model.eval()
       correct = 0
       total = 0
       all_predictions = []
       all_targets = []
      
       with torch.no_grad():
           for data, target in X_test_loader:
               data = data.to(device)
               outputs = model(data)
               _, predicted = torch.max(outputs.data, 1)
               total += target.size(0)
               correct += (predicted == target).sum().item()
               all_predictions.extend(predicted.cpu().numpy())
               all_targets.extend(target.cpu().numpy())
      
       accuracy = 100 * correct / total
      
       # TODO: Add more evaluation metrics
       # Hint: Use classification_report, confusion_matrix from sklearn.metrics
        # Print or return precision, recall, F1-score for each class
       cm = confusion_matrix(target, predicted)
     
       disp = ConfusionMatrixDisplay(cm)
       disp.plot()
       plt.show() 
       print("Classification Report \n", classification_report(target, predicted))
   
       return accuracy

# 4. Placeholder for main training pipeline
def main():
   # Load training data
   df = load_data("train.csv")
   # split data, train_df is your train data.
   df, test_df = train_test_split(df, test_size=0.15, random_state=1)

   df = feature_processing(df)

   # Split features and labels
   X = df.drop("price_range", axis=1)
   y = df["price_range"]
  
   # Preprocessing (e.g., scaling) â€” add later
  
   scaler = StandardScaler()  # TODO: You need to read sklearn document.
   # https://scikit-learn.org/stable/data_transforms.html
   X_scaled = scaler.fit_transform(X)
   X_scaled = pd.DataFrame(X_scaled, columns=X.columns)


   # TODO: Train/val split
   # X_train, X_test, y_train, y_test
   X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=462)
   # https://scikit-learn.org/0.19/modules/generated/sklearn.model_selection.train_test_split.html
  


   # Prepare datasets and loaders
   train_dataset = MobilePriceDataset(X_train, y_train)
   val_dataset = MobilePriceDataset(X_val, y_val)


   train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
   val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)


   # Define model
   model = MLPClassifier(input_dim=X.shape[1])


   # Move model to device
   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
   # For Mac M-series chips, you can also try:
   # device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
   print(f"Using device: {device}")


   model = model.to(device)


   # Here is a basic training loop
   # Training setup
   criterion = nn.CrossEntropyLoss()
   optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5) #  , weight_decay=1e-4You can play with different optimizer, added weight decay
   scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)
  
   # Training parameters
   num_epochs = 150 # You can change this number


   # The following three lines are related to early stop
   #TODO: Apply early stop for your model
  
   early_stop_patience = 15
   with open("best_val_loss.txt") as f:
       if f.read() == None :
           best_val_loss = float('inf')
       else : 
        best_val_loss =  f.read()
#    best_val_loss = float('inf')
   patience_counter = 0




   #########################################################
  
   # Lists to store metrics for learning curves
   train_losses = []
   val_losses = []
   train_accuracies = []
   val_accuracies = []
  
   print(f"Training on device: {device}")
   print(f"Model has {sum(p.numel() for p in model.parameters())} parameters")
  
   
   # Training loop
   for epoch in range(num_epochs):
       # Training phase
       model.train()
       train_loss = 0.0
       train_correct = 0
       train_total = 0
       
       for batch_idx, (data, target) in enumerate(train_loader):
           data, target = data.to(device), target.to(device) # move data to gpu
          
           # Zero gradients before every batch
           optimizer.zero_grad()
          
           # Forward pass
           outputs = model(data)
           loss = criterion(outputs, target)
          
           # Backward pass
           loss.backward()
           optimizer.step()
          
           # Statistics
           train_loss += loss.item()
           _, predicted = torch.max(outputs.data, 1)
           train_total += target.size(0)
           train_correct += (predicted == target).sum().item()
      
       # Calculate training metrics
       avg_train_loss = train_loss / len(train_loader)
       train_accuracy = 100 * train_correct / train_total
      
       # Validation phase
       model.eval()
       val_loss = 0.0
       val_correct = 0
       val_total = 0
      
       with torch.no_grad():
           for data, target in val_loader:
               data, target = data.to(device), target.to(device)
               outputs = model(data)
               loss = criterion(outputs, target)
              
               val_loss += loss.item()
               _, predicted = torch.max(outputs.data, 1) # convert proability to 0/1 label
              
               val_total += target.size(0)
               val_correct += (predicted == target).sum().item()
      
       # Calculate validation metrics
       avg_val_loss = val_loss / len(val_loader)
       val_accuracy = 100 * val_correct / val_total
      
       # Store metrics for learning curves
       train_losses.append(avg_train_loss)
       val_losses.append(avg_val_loss)
       train_accuracies.append(train_accuracy)
       val_accuracies.append(val_accuracy)
      
       # TODO: Learning rate scheduling
       scheduler.step(val_loss)
      
      
       # Print progress
       if (epoch + 1) % 10 == 0 or epoch == 0:
           print(f'Epoch [{epoch+1}/{num_epochs}]')
           print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%')
           print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')
           print(f'LR: {optimizer.param_groups[0]["lr"]:.6f}')
           print('-' * 50)


       # TODO: Early stopping
       if val_loss < best_val_loss:
           best_val_loss = val_loss
           patience_counter = 0
           torch.save(model.state_dict(), 'best_model.pt')
       else:
           patience_counter += 1


       if patience_counter >= early_stop_patience:
           print("Early stopping!")
           break
       
       
     


       # TODO: save best model
       # uncomment the following line after you apply early stopping
       # torch.save(model.state_dict(), 'best_model.pt')
      
  
   plot_learning_curves(train_losses, val_losses, train_accuracies, val_accuracies)
  
   
   
   accuracy = predict(model, train_loader, device)
   print("Current accuracy is :" , accuracy)
#    cm = confusion_matrix(y_val, predicted)
#    disp = ConfusionMatrixDisplay(cm)
#    disp.plot()
#    print("Classification Report \n", classification_report(y_test, y_pred))
   
  
       
       
   return model, scaler




def plot_learning_curves(train_losses, val_losses, train_accuracies, val_accuracies):
   # TODO: Use this function to plot learning curve
   """Plot learning curves for loss and accuracy"""
   import matplotlib.pyplot as plt
  
   epochs = range(1, len(train_losses) + 1)
  
   fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
   # ax3.plot(epochs, train_losses, label="Training Loss")
   # ax3.plot(epochs, val_losses, label="Validation Loss")
   # ax3.set_xlabel("Epoch")
   # ax3.set_ylabel("Loss")
   # ax3.set_title("Learning Curve")
   # ax3.legend(fontsize=12)
   # ax3.grid(True)
   # plt.show()


   # Plot losses
   ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)
   ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)
   ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
   ax1.set_xlabel('Epoch', fontsize=12)
   ax1.set_ylabel('Loss', fontsize=12)
   ax1.legend(fontsize=12)
   ax1.grid(True, alpha=0.3)
  
   # Plot accuracies
   ax2.plot(epochs, train_accuracies, 'b-', label='Training Accuracy', linewidth=2)
   ax2.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)
   ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
   ax2.set_xlabel('Epoch', fontsize=12)
   ax2.set_ylabel('Accuracy (%)', fontsize=12)
   ax2.legend(fontsize=12)
   ax2.grid(True, alpha=0.3)
  
   plt.tight_layout()
   plt.show(block=True)




if __name__ == "__main__":
   model, scaler = main()


   # load the best model
   model.load_state_dict(torch.load('best_model.pt'))
  
  

   def predict(model, X_test_loader, device):
       """
       Evaluate model on test data and return comprehensive metrics
       """
       model.eval()
       correct = 0
       total = 0
       all_predictions = []
       all_targets = []
      
       with torch.no_grad():
           for data in X_test_loader:
               data = data.to(device)
               outputs = model(data)
               _, predicted = torch.max(outputs.data, 1)
               total += target.size(0)
               correct += (predicted == target).sum().item()
              
               all_predictions.extend(predicted.cpu().numpy())
               all_targets.extend(target.cpu().numpy())
      
       accuracy = 100 * correct / total
      
       # TODO: Add more evaluation metrics
       # Hint: Use classification_report, confusion_matrix from sklearn.metrics
       # Print or return precision, recall, F1-score for each class
    #    cm = confusion_matrix(y_val, predicted)
    #    disp = ConfusionMatrixDisplay(cm)
    #    disp.plot()
    #print("Classification Report \n", classification_report(y_test, y_pred))
   
       return accuracy